{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b27085a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "#!pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e71a151",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql.types import StructType, StructField,StringType, IntegerType, DateType\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "import pathlib\n",
    "import os\n",
    "import sys\n",
    "from utils import *\n",
    "from archiver.index import *\n",
    "from calcul_conso.index import *\n",
    "from calcul_stats.index import *\n",
    "from filtrages.apply_facteur import *\n",
    "from filtrages.index import *\n",
    "from headers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23dd673a",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient('localhost',27017)\n",
    "db = client['sentinel-dev']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2d780b",
   "metadata": {},
   "source": [
    "def func(batch_df, batch_id):\n",
    "    wanted_df = ps.DataFrame(batch_df)\n",
    "    wanted_df['site']='Test1'\n",
    "    wanted_df['compteur']='01'\n",
    "    \n",
    "    # wanted_df['path']= f\"{pathlib.Path(__file__).parent.resolve()}\" # un comment in pyfiles\n",
    "    wanted_df['path'] = os.getcwd() #comment in pyfiles\n",
    "    wanted_df = wanted_df.dropna()\n",
    "    records = json.loads(wanted_df.to_json())\n",
    "    db['test-pyspark9'].insert_many(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38f22ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(batch_df,batchid):\n",
    "    wanted_df = ps.DataFrame(batch_df.toPandas())\n",
    "       \n",
    "    complement = getComplement('/home/pape/sentinel-hybrid/ftp/ONAS/ONAS TECHNOPOLE/01')\n",
    "   \n",
    "    wanted_df = add_complement(complement,wanted_df)\n",
    "    \n",
    "    wanted_df = apply_facteur(wanted_df)\n",
    "    \n",
    "    wanted_df= apply_filter(wanted_df)\n",
    "    \n",
    "    energie_hour_array = get_energies(wanted_df)\n",
    "\n",
    "    stats = get_stats(wanted_df,energie_hour_array)\n",
    "    insert_to_stats(stats)\n",
    "    \n",
    "    insert_to_mongo(wanted_df)\n",
    "    \n",
    "    archiver('/home/pape/sentinel-hybrid/ftp/ONAS/ONAS TECHNOPOLE/05','/home/pape/sentinel-hybrid/ftp-archive/ONAS/ONAS TECHNOPOLE/05','/home/pape/sentinel-hybrid/ftp-archive/ONAS/ONAS TECHNOPOLE/05')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f5db10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/01 16:49:10 WARN Utils: Your hostname, pape-Lenovo-IdeaPad-L340-15IWL resolves to a loopback address: 127.0.1.1; using 192.168.0.122 instead (on interface wlp0s20f3)\n",
      "22/07/01 16:49:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/spark-3.2.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/07/01 16:49:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "sc = SparkSession.builder.appName('Test8').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e7c1bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_standard = StructType([     StructField('jour', IntegerType(),True),     StructField('mois', IntegerType(),True),     StructField('annee', IntegerType(),True),     StructField('heure', IntegerType(),True),     StructField('minutes', IntegerType(),True),     StructField('secondes', IntegerType(),True),     StructField('energie_active_secteur', IntegerType(),True),     StructField('energie_reactive_secteur', IntegerType(),True),     StructField('puissance_mesuree', IntegerType(),True),     StructField('puissance_reactive', IntegerType(),True),     StructField('intensite_1_mesuree', IntegerType(),True),     StructField('intensite_2_mesuree', IntegerType(),True),     StructField('intensite_3_mesuree', IntegerType(),True),     StructField('intensite', IntegerType(),True),     StructField('u_l1', IntegerType(),True),     StructField('u_l2', IntegerType(),True),     StructField('u_l3', IntegerType(),True),     StructField('p_1', IntegerType(),True),     StructField('p_2', IntegerType(),True),     StructField('p_3', IntegerType(),True),     StructField('thd_u1', IntegerType(),True),     StructField('thd_u2', IntegerType(),True),     StructField('thd_u3', IntegerType(),True),     StructField('thd_i1', IntegerType(),True),     StructField('thd_i2', IntegerType(),True),     StructField('thd_i3', IntegerType(),True),     StructField('etat', IntegerType(),True),     StructField('i_neutre', IntegerType(),True)\n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8f104d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_onas = StructType([\n",
    "    StructField('date', StringType(),True), \\\n",
    "    StructField('heure',StringType(),True), \\\n",
    "    StructField('e_active', IntegerType(),True), \\\n",
    "    StructField('e_reactive',IntegerType(),True), \\\n",
    "    StructField('p_active', IntegerType(),True), \\\n",
    "    StructField('p_reactive',IntegerType(),True), \\\n",
    "    StructField('i_1', IntegerType(),True), \\\n",
    "    StructField('i_2',IntegerType(),True), \\\n",
    "    StructField('i_3',IntegerType(),True), \\\n",
    "    StructField('i',IntegerType(),True), \\\n",
    "    StructField('u_1', IntegerType(),True), \\\n",
    "    StructField('u_2',IntegerType(),True), \\\n",
    "    StructField('u_3',IntegerType(),True), \\\n",
    "    StructField('p_1', IntegerType(),True), \\\n",
    "    StructField('p_2',IntegerType(),True), \\\n",
    "    StructField('p_3',IntegerType(),True), \\\n",
    "    StructField('cos_phi', IntegerType(),True), \\\n",
    "    StructField('u_l1', IntegerType(),True), \\\n",
    "    StructField('u_l2',IntegerType(),True), \\\n",
    "    StructField('u_l3',IntegerType(),True), \\\n",
    "    StructField('thd_u1', IntegerType(),True), \\\n",
    "    StructField('thd_u2',IntegerType(),True), \\\n",
    "    StructField('thd_u3',IntegerType(),True), \\\n",
    "    StructField('thd_i1', IntegerType(),True), \\\n",
    "    StructField('thd_i2',IntegerType(),True), \\\n",
    "    StructField('thd_i3',IntegerType(),True), \\\n",
    "    StructField('i_neutre',IntegerType(),True), \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f197d46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_box = StructType([\n",
    "    StructField('jour', IntegerType(),True),  \\\n",
    "    StructField('mois', IntegerType(),True),  \\\n",
    "    StructField('annee', IntegerType(),True), \\\n",
    "    StructField('heure', IntegerType(),True), \\\n",
    "    StructField('minutes', IntegerType(),True), \\\n",
    "    StructField('secondes', IntegerType(),True), \\\n",
    "    StructField('energie_active_secteur', IntegerType(),True), \\\n",
    "    StructField('energie_reactive_secteur', IntegerType(),True), \\\n",
    "    StructField('puissance_mesuree', IntegerType(),True), \\\n",
    "    StructField('puissance_reactive', IntegerType(),True), \\\n",
    "    StructField('intensite_1_mesuree', IntegerType(),True), \\\n",
    "    StructField('intensite_2_mesuree', IntegerType(),True), \\\n",
    "    StructField('intensite_3_mesuree', IntegerType(),True), \\\n",
    "    StructField('intensite', IntegerType(),True), \\\n",
    "    StructField('tension_1_mesuree', IntegerType(),True), \\\n",
    "    StructField('tension_2_mesuree', IntegerType(),True), \\\n",
    "    StructField('tension_3_mesuree', IntegerType(),True), \\\n",
    "    StructField('p_1', IntegerType(),True), \\\n",
    "    StructField('p_2', IntegerType(),True), \\\n",
    "    StructField('p_3', IntegerType(),True), \\\n",
    "    StructField('cos_phi', IntegerType(),True), \\\n",
    "    StructField('u_l1', IntegerType(),True), \\\n",
    "    StructField('u_l2', IntegerType(),True), \\\n",
    "    StructField('u_l3', IntegerType(),True), \\\n",
    "    StructField('thd_u1', IntegerType(),True), \\\n",
    "    StructField('thd_u2', IntegerType(),True), \\\n",
    "    StructField('thd_u3', IntegerType(),True), \\\n",
    "    StructField('thd_i1', IntegerType(),True), \\\n",
    "    StructField('thd_i2', IntegerType(),True), \\\n",
    "    StructField('thd_i3', IntegerType(),True), \\\n",
    "    StructField('i_neutre', IntegerType(),True), \\\n",
    "    StructField('site', StringType(),True), \\\n",
    "    StructField('i_neutre', IntegerType(),True)\n",
    " ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87503ac6",
   "metadata": {},
   "source": [
    "## The schema need to be provided before run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3224e9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = sc.readStream.format('csv').option('path',\"/home/pape/Downloads/onas/01\").schema(schema_onas).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b466fdc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/01 16:49:20 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-c9e4bd75-a12c-4f3a-9967-86cb65f6e028. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/07/01 16:49:20 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "22/07/01 16:49:41 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ssc.writeStream.format('console').foreachBatch(func).outputMode('append').start().awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f930ab-706d-47ea-b047-b39dcae7c5d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
